\documentclass[12pt, a4paper]{article}
\PassOptionsToPackage{sharp}{prettytex/boxes}
\usepackage{prettytex/base}

\setlength{\topmargin}{0.0in}
\setlength{\oddsidemargin}{0.33in}
\setlength{\textheight}{9.0in}
\setlength{\textwidth}{6.0in}
\renewcommand{\baselinestretch}{1.25}

\usepackage{prettytex/math}
\usepackage[nameinlink]{cleveref}
\usepackage[cleveref]{prettytex/math-theorems}
\usepackage{prettytex/mathematicians}
% \usepackage{prettytex/gfx}
% \usepackage{prettytex/code}
\usepackage{prettytex/pseudo}
\usepackage{prettytex/thesis}
\usepackage{bbm}

\setlength{\headheight}{19.53pt}
\setlength{\headsep}{1.8em}
\setlength{\belowcaptionskip}{-12pt}
\addbibresource{sources.bib}
% \setminted{fontsize=\footnotesize}
\AfterEndEnvironment{minted}{\vspace*{-0.8cm}}
% \tikzexternalize[prefix=tikz/]
\renewcommand{\operatorcolor}{black}

\newcommand{\chebyshev}{Chebyshev\xspace}
\newcommand{\tschebfun}{\textcolor{themecolor3}{TschebFun}\xspace}
\newcommand{\heatfun}{\textcolor{themecolor3}{HeatFun}\xspace}
\newcommand{\identity}{\mathbbm{1}}

\newcommand{\topictitle}{Unsupervised Semantic Field Analysis \\ \large by methods from community detection}
\newcommand{\candidatenumber}{1072462}
\newcommand{\course}{Networks}

\title{\topictitle}
\author{Candidate \candidatenumber}
\date{\today}

% âœ… Prepare report
% Insert a few pictures from references maybe
% Try out simulated Annealing
% in the introduction, mention all the methods described by Fortunato et al. (lots of text)

\begin{document}
  \pagestyle{plain}
  \begin{center}
    \vspace*{-2.5cm}
    \Large \topictitle \\
    \vspace{.3cm}

    \normalsize Special Topic on \textcolor{themecolor3}{\textsc{\course}}\\
    \normalsize Candidate Number: \textcolor{themecolor3}{\candidatenumber}
    \vspace{.3cm}
  \end{center}

  \begin{abstract}
    \label{abstract}
    This work will attempt to
  \end{abstract}

  \begin{figure}[H]
    \centering
    \caption{Add some sort of graph plot here.}
  \end{figure}

  \pagebreak
  \pagestyle{normal}

  % \tableofcontents
  % \pagebreak

  \section{Motivation}
  For non-periodic problem settings, \chebyshev series are a fantastic choice \parencite{cw-biemann}.

  \pagebreak
  \section{Introduction}
  Let $\N = \Z^+$ denote the positive integers and $N_0 := \{0\} \cup \N$ the nonnegative integers.

  The methods we will discuss to identify semantic fields will be based on graph clustering algorithms applied to a text corpus word connectedness / neighbourhood network.
  As we will discuss later, different notions of connectedness can give us different insight into the structure of a natural language.
  We will focus our attention on methods for undirected graphs, ``graphs without direction'' (cf. \Cref{def:undirected-graph}).
  \begin{definition}{Undirected Graph}{undirected-graph}
    A graph $G = (V, E)$ with vertices $V$ and edges $E \subseteq V \times V$ is undirected if and only if $(v_i, v_j) \in E \Rightarrow (v_j, v_i) \in E \quad \forall\; v_i, v_j \in V$.
  \end{definition}

  Vertices are also often referred to as \textit{nodes}.
  Every graph $G$ is uniquely described by its adjacency matrix $A \in \{0, 1\}^{n \times n}$ (\Cref{def:adjacency-matrix}), which allows us to talk about ``linear algebra'' of graphs.

  \begin{definition}{Adjacency Matrix}{adjacency-matrix}
    Let $A \in \{0, 1\}^{n \times n}$ denote the symmetric adjacency matrix of an undirected graph $G = (V, E)$. Its entries are given by $a_{ij} = \{A\}_{ij} = \identity_{(v_i, v_j) \in E}$, so $a_{ij} = 1$ if vertex $v_i$ is connected to $v_j$ and $0$ otherwise.
  \end{definition}

  By construction, $A = A^T$ is symmetric and has all-$0$s in the diagonal, a definition that corresponds to the fact that you cannot be friends with yourself in a social network.

  Further let $m := |E|$ and $n := |V|$ denote the number of edges and vertices, respectively.
  The degree $d_i$ of a vertex $v_i \in V$ is defined by the number of edges connecting to it, so $$d_i := \deg(v_i) = \big|\left\{(v_j, v_k) \in E \;|\; v_j = v_i\right\}\big|\,,$$ for an undirected graph $G = (V, E)$. The handshaking lemma (\Cref{lemma:handshaking}) tells us an important fact useful for normalisation.
  \begin{lemma}{Handshake}{handshaking}
    For every finite, undirected graph $G = (V, E)$ the individual vertex degrees sum up to exactly twice the number of edges, so $$\sum_{i=1}^{n} d_i = \sum_{v \in V} \deg(v) = 2m\,.$$
  \end{lemma}

  The individual vertex degrees can be summarised in the so-called \textit{degree matrix} $D := \diag(d_1, ..., d_n)$, $D \in \N_0^{n \times n}$. The graph \textit{Laplacian} is defined by $L := D - A$.
  % TODO: need this matrix?

  Given a graph, we are interested in performing \textbf{graph clustering}, also referred to as \textbf{community detection} or \textbf{graph partitioning}, the goal of which is to obtain a set of mutually exclusive clusters $C_i \subseteq V$ (cf. \Cref{def:clustering}).
  The term \textit{graph partitioning} is more frequently used in the context of minimal cuts, where one aims to minimise each \textit{cut size} referring to the number of edges in between clusters.

  \begin{definition}{Graph Clustering}{clustering}
    Let $C = \left\{C_i \subseteq V \right\}_{i=1 ... n_C}$ denote a clustering of $G = (V, E)$ into $n_C \in \N$ clusters where $C_i \cap C_j = \{\} \; \forall\,i, j \in \{1, ..., n_C\}$ and $\bigcup_{i=1}^{n_C} C_i = V$. Let $s_i \in \{1, ..., n_C\}$ denote the assigned cluster of vertex $v_i \in V$.
  \end{definition}

  These clusterings may be better or worse depending on the context, but a generally solid measure of ``clustering goodness'' is \textit{modularity} (\Cref{def:modularity}).

  \begin{definition}{Modularity}{modularity}
    For a given undirected graph $G = (V, E)$ and clustering $C$, let $$Q := \frac{1}{2m} \sum_{i=1}^{n} \sum_{j=1}^{n} \left(A_{ij} - \frac{d_i d_j}{2m}\right) \delta(s_i, s_j)\,,$$ with $\delta(\cdot, \cdot)$ the Kronecker delta indicating whether two vertices $v_i$ and $v_j$ belong to the same cluster \parencite{lambiotte-louvain-clustering}.
  \end{definition}

  Modularity is a measure of the quality of a clustering (also referred to as a partitioning) of $G$. It can also be written as
  $$Q = \frac{1}{2m} \sum_{c=1}^{n_C} \left[\sum_{v_i, v_j \in C_i} \left(A_{ij} - \frac{d_i d_j}{2m}\right)\right]\,,$$
  which might make its purpose a bit clearer.

  \section{Clustering Methods and Algorithms}
  In order to find ...

  Community detection is, in principle, usually a ``very hard'' task given the vast number of possible system configurations as the graph grows in the number of edges or vertices, a statement which can be made more precise using complexity theory.
  In conventional complexity theory, problems are filed into different complexity classes when analysing their runtime and memory usage.
  There exist
  \begin{enumerate}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item NL (Nondeterministic Logarithmic space)
    \item P (Polynomial time)
    \item NP (Nondeterministic Polynomial time)
    \item PSPACE (Polynomial space)
    \item EXPTIME (Exponential time)
    \item EXPSPACE (Exponential space)
  \end{enumerate}
  computational complexity classes, sorted by the amount of problems contained in them (NL $\subseteq$ P $\subseteq$ NP $\subseteq$ PSPACE $\subseteq$ EXPTIME $\subseteq$ EXPSPACE).
  A particularly interesting open problem is whether P = NP, one of the millennium prize problems and the most important open problem in computer science.

  \begin{definition}{NP-Hardness}{np-hard}
    A problem is referred to as \textit{NP-hard} if and only if it is at least as hard as the hardest problems in the complexity class NP (nondeterministic polynomial time). Formally written,
    $$\mathrm{NP} := \bigcup_{k \in \N} \mathrm{NTIME}(n^k)$$
    the union of all decision problems with runtime bounded by $\mathcal{O}(n^k)$.
  \end{definition}

  Many community detection algorithms or problems relating to it are NP-hard \parencite{fortunato}.
  It is therefore often futile to employ exact algorithms as they quickly start to become infeasible for larger system sizes.

  \subsection{Embeddings}
  Cosine-Similarity

  \subsection{Girvan-Newman}
  \subsection{Louvain}
  \cite{grindrod-lecture-notes}.
  \cite{lambiotte-louvain-clustering}.

  % https://en.wikipedia.org/wiki/Louvain_method

  \subsection{Chinese Whispers}
  \begin{algorithm}[language=pseudo, caption={\centering The \textit{Chinese Whispers} algorithm due to \cite{cw-biemann}}]
Input: an undirected graph $G = (V, E)$.
Output: a graph clustering $C = \{C_i\}_{i=1, ..., n_C}$ into $n_C$ classes.

Initialise the algorithm with $n_C = n$ classes, one per vertex.
while there are changes or the iteration maximum is reached, do
  for $v_i$ in shuffle($V$), do
    Set $s_i = 3$
  end
end
  \end{algorithm}

  Corresponds to an agent-based simulation of a social network
  \cite{cw-biemann}.

  \subsection{Watset}
  \cite{watset}.

  \subsection{Spectral Clustering Methods}
  \cite{fortunato}.

  \subsection{Fiedler}
  \cite{fortunato}.

  \section{Results}
  An itemize with some semantic field clusters we found

  Plotty plot

  Apply to Karate Club

  Computational complexity table from \cite{watset}, also referencing complexity section above.

  \section{Author analysis?}
  Authors whose works circulate around these semantic fields:
  bla bla, maybe not that interesting

  \pagebreak
  \section{Discussion and Outlook}
  Bla

  \pagebreak
  \printbibliography

  \appendix
  \section{Appendix Things?}
\end{document}
